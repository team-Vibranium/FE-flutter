import 'dart:async';
import 'dart:convert';
import 'dart:typed_data';
import 'package:flutter/foundation.dart';
import 'package:flutter_webrtc/flutter_webrtc.dart';
import 'package:web_socket_channel/web_socket_channel.dart';
import 'package:web_socket_channel/io.dart';
import 'package:permission_handler/permission_handler.dart';

/// GPT Realtime APIë¥¼ í™œìš©í•œ ì–‘ë°©í–¥ ìŒì„± ëŒ€í™” ì„œë¹„ìŠ¤
class GPTRealtimeService {
  static final GPTRealtimeService _instance = GPTRealtimeService._internal();
  factory GPTRealtimeService() => _instance;
  GPTRealtimeService._internal();

  // WebSocket ì—°ê²°
  WebSocketChannel? _channel;
  
  // WebRTC ê´€ë ¨
  RTCPeerConnection? _peerConnection;
  MediaStream? _localStream;
  MediaStream? _remoteStream;
  
  // ìƒíƒœ ê´€ë¦¬
  bool _isConnected = false;
  bool _isCallActive = false;
  String? _sessionId;
  
  // ì½œë°±
  Function(String)? onMessageReceived;
  Function(String)? onError;
  Function()? onCallStarted;
  Function()? onCallEnded;
  Function(MediaStream)? onRemoteStream;

  // GPT Realtime API ì„¤ì •
  static const String _gptRealtimeUrl = 'wss://api.openai.com/v1/realtime';
  String? _apiKey;

  /// ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
  Future<void> initialize(String apiKey) async {
    _apiKey = apiKey;
    
    // ê¶Œí•œ ìš”ì²­
    await _requestPermissions();
    
    print('ğŸ¤ GPT Realtime Service ì´ˆê¸°í™” ì™„ë£Œ');
  }

  /// ê¶Œí•œ ìš”ì²­
  Future<void> _requestPermissions() async {
    final microphoneStatus = await Permission.microphone.request();
    
    if (microphoneStatus != PermissionStatus.granted) {
      throw Exception('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤');
    }
    
    print('âœ… ë§ˆì´í¬ ê¶Œí•œ ìŠ¹ì¸ë¨');
  }

  /// ëª¨ë‹ì½œ ì‹œì‘
  Future<void> startMorningCall({
    required String alarmTitle,
    required String userName,
  }) async {
    try {
      if (_apiKey == null) {
        throw Exception('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
      }

      print('ğŸŒ… ëª¨ë‹ì½œ ì‹œì‘: $alarmTitle');
      
      // 1. WebRTC ì´ˆê¸°í™”
      await _initializeWebRTC();
      
      // 2. GPT Realtime API ì—°ê²°
      await _connectToGPT();
      
      // 3. ì„¸ì…˜ ì„¤ì •
      await _setupSession(alarmTitle, userName);
      
      // 4. ëŒ€í™” ì‹œì‘
      await _startConversation(alarmTitle, userName);
      
      _isCallActive = true;
      onCallStarted?.call();
      
    } catch (e) {
      print('âŒ ëª¨ë‹ì½œ ì‹œì‘ ì˜¤ë¥˜: $e');
      onError?.call('ëª¨ë‹ì½œ ì‹œì‘ ì‹¤íŒ¨: $e');
      rethrow;
    }
  }

  /// WebRTC ì´ˆê¸°í™”
  Future<void> _initializeWebRTC() async {
    // WebRTC ì„¤ì •
    final configuration = {
      'iceServers': [
        {'urls': 'stun:stun.l.google.com:19302'},
      ],
    };

    // PeerConnection ìƒì„±
    _peerConnection = await createPeerConnection(configuration);
    
    // ë¡œì»¬ ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ íšë“ (Flutter WebRTC ë°©ì‹)
    Map<String, dynamic> mediaConstraints = {
      'audio': {
        'echoCancellation': true,
        'noiseSuppression': true,
        'autoGainControl': true,
        'sampleRate': 24000, // GPT Realtime API ìš”êµ¬ì‚¬í•­
        'channelCount': 1,
      },
      'video': false,
    };
    
    _localStream = await MediaDevices.getUserMedia(mediaConstraints);

    // ë¡œì»¬ ìŠ¤íŠ¸ë¦¼ì„ PeerConnectionì— ì¶”ê°€
    _localStream!.getTracks().forEach((track) {
      _peerConnection!.addTrack(track, _localStream!);
    });

    // ì›ê²© ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
    _peerConnection!.onAddStream = (stream) {
      _remoteStream = stream;
      onRemoteStream?.call(stream);
      print('ğŸ”Š ì›ê²© ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ìˆ˜ì‹ ë¨');
    };

    print('ğŸ§ WebRTC ì´ˆê¸°í™” ì™„ë£Œ');
  }

  /// GPT Realtime API ì—°ê²°
  Future<void> _connectToGPT() async {
    final headers = {
      'Authorization': 'Bearer $_apiKey',
      'OpenAI-Beta': 'realtime=v1',
    };

    try {
      _channel = IOWebSocketChannel.connect(
        Uri.parse('$_gptRealtimeUrl?model=gpt-4o-realtime-preview-2024-12-17'),
        headers: headers,
      );

      // WebSocket ë©”ì‹œì§€ ì²˜ë¦¬
      _channel!.stream.listen(
        _handleWebSocketMessage,
        onError: (error) {
          print('âŒ WebSocket ì˜¤ë¥˜: $error');
          onError?.call('GPT ì—°ê²° ì˜¤ë¥˜: $error');
        },
        onDone: () {
          print('ğŸ”Œ GPT ì—°ê²° ì¢…ë£Œë¨');
          _isConnected = false;
        },
      );

      _isConnected = true;
      print('ğŸ¤– GPT Realtime API ì—°ê²°ë¨');
      
    } catch (e) {
      throw Exception('GPT API ì—°ê²° ì‹¤íŒ¨: $e');
    }
  }

  /// WebSocket ë©”ì‹œì§€ ì²˜ë¦¬
  void _handleWebSocketMessage(dynamic message) {
    try {
      final data = jsonDecode(message);
      final type = data['type'] as String?;
      
      print('ğŸ“¨ GPT ë©”ì‹œì§€: $type');
      
      switch (type) {
        case 'session.created':
          _sessionId = data['session']['id'];
          print('âœ… GPT ì„¸ì…˜ ìƒì„±ë¨: $_sessionId');
          break;
          
        case 'response.audio.delta':
          // ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
          _handleAudioResponse(data);
          break;
          
        case 'response.text.delta':
          // í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
          final text = data['delta'] as String?;
          if (text != null) {
            onMessageReceived?.call(text);
          }
          break;
          
        case 'error':
          final error = data['error']['message'];
          print('âŒ GPT ì˜¤ë¥˜: $error');
          onError?.call('GPT ì˜¤ë¥˜: $error');
          break;
          
        default:
          print('ğŸ” ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë©”ì‹œì§€ íƒ€ì…: $type');
      }
    } catch (e) {
      print('âŒ ë©”ì‹œì§€ íŒŒì‹± ì˜¤ë¥˜: $e');
    }
  }

  /// ì˜¤ë””ì˜¤ ì‘ë‹µ ì²˜ë¦¬
  void _handleAudioResponse(Map<String, dynamic> data) {
    try {
      final audioData = data['delta'] as String?;
      if (audioData != null && _remoteStream != null) {
        // Base64 ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë””ì½”ë”©í•˜ì—¬ ì¬ìƒ
        final audioBytes = base64Decode(audioData);
        _playAudioData(audioBytes);
      }
    } catch (e) {
      print('âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: $e');
    }
  }

  /// ì˜¤ë””ì˜¤ ë°ì´í„° ì¬ìƒ
  void _playAudioData(Uint8List audioBytes) {
    // WebRTCë¥¼ í†µí•´ ì˜¤ë””ì˜¤ ì¬ìƒ
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì˜¤ë””ì˜¤ íŠ¸ë™ì— ì§ì ‘ ë°ì´í„°ë¥¼ ì£¼ì…í•´ì•¼ í•¨
    print('ğŸ”Š ì˜¤ë””ì˜¤ ì¬ìƒ: ${audioBytes.length} bytes');
  }

  /// ì„¸ì…˜ ì„¤ì •
  Future<void> _setupSession(String alarmTitle, String userName) async {
    final sessionConfig = {
      'type': 'session.update',
      'session': {
        'modalities': ['text', 'audio'],
        'instructions': '''
ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì—ë„ˆì§€ ë„˜ì¹˜ëŠ” AI ëª¨ë‹ì½œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì´ë¦„: $userName
ì˜¤ëŠ˜ì˜ ì•ŒëŒ: $alarmTitle

ì—­í• :
1. $userNameì„ ìƒëƒ¥í•˜ê³  í™œê¸°ì°¨ê²Œ ê¹¨ì›Œì£¼ì„¸ìš”
2. ì•ŒëŒ ì œëª© "$alarmTitle"ê³¼ ê´€ë ¨ëœ ë™ê¸°ë¶€ì—¬ ë©˜íŠ¸ë¥¼ í•´ì£¼ì„¸ìš”
3. ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ê²©ë ¤í•´ì£¼ì„¸ìš”
4. ì‚¬ìš©ìê°€ ì™„ì „íˆ ê¹¨ì–´ë‚  ë•Œê¹Œì§€ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”
5. ê°„ë‹¨í•œ ìŠ¤íŠ¸ë ˆì¹­ì´ë‚˜ ê¸°ìƒ ë£¨í‹´ì„ ì œì•ˆí•´ì£¼ì„¸ìš”

ë§íˆ¬: ì¹œê·¼í•˜ê³  ë°ì€ í†¤ìœ¼ë¡œ, ë§ˆì¹˜ ì¹œí•œ ì¹œêµ¬ê°€ ê¹¨ì›Œì£¼ëŠ” ê²ƒì²˜ëŸ¼
''',
        'voice': 'alloy', // ë˜ëŠ” 'echo', 'fable', 'onyx', 'nova', 'shimmer'
        'input_audio_format': 'pcm16',
        'output_audio_format': 'pcm16',
        'input_audio_transcription': {
          'model': 'whisper-1'
        },
        'turn_detection': {
          'type': 'server_vad',
          'threshold': 0.5,
          'prefix_padding_ms': 300,
          'silence_duration_ms': 500,
        },
        'tools': [],
        'tool_choice': 'auto',
        'temperature': 0.8,
      }
    };

    _sendMessage(sessionConfig);
    await Future.delayed(const Duration(milliseconds: 500));
  }

  /// ëŒ€í™” ì‹œì‘
  Future<void> _startConversation(String alarmTitle, String userName) async {
    final startMessage = {
      'type': 'conversation.item.create',
      'item': {
        'type': 'message',
        'role': 'user',
        'content': [
          {
            'type': 'input_text',
            'text': 'ì•ˆë…•! ì•ŒëŒ ì œëª©ì€ "$alarmTitle"ì´ì•¼. ì´ê±¸ ê¸°ë°˜ìœ¼ë¡œ $userNameì„ ê¹¨ì›Œì¤˜!'
          }
        ]
      }
    };

    _sendMessage(startMessage);

    // ì‘ë‹µ ìƒì„± ìš”ì²­
    final responseRequest = {
      'type': 'response.create',
      'response': {
        'modalities': ['text', 'audio'],
        'instructions': 'ì‚¬ìš©ìë¥¼ í™œê¸°ì°¨ê²Œ ê¹¨ì›Œì£¼ì„¸ìš”!'
      }
    };

    _sendMessage(responseRequest);
  }

  /// ë©”ì‹œì§€ ì „ì†¡
  void _sendMessage(Map<String, dynamic> message) {
    if (_channel != null && _isConnected) {
      _channel!.sink.add(jsonEncode(message));
      print('ğŸ“¤ GPTë¡œ ë©”ì‹œì§€ ì „ì†¡: ${message['type']}');
    } else {
      print('âŒ GPT ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤');
    }
  }

  /// ì‚¬ìš©ì ìŒì„± ì…ë ¥ ì „ì†¡
  void sendAudioInput(Uint8List audioData) {
    if (!_isConnected) return;

    final audioMessage = {
      'type': 'input_audio_buffer.append',
      'audio': base64Encode(audioData),
    };

    _sendMessage(audioMessage);
  }

  /// ìŒì„± ì…ë ¥ ì™„ë£Œ ì‹ í˜¸
  void commitAudioInput() {
    if (!_isConnected) return;

    _sendMessage({'type': 'input_audio_buffer.commit'});
    _sendMessage({'type': 'response.create'});
  }

  /// ëª¨ë‹ì½œ ì¢…ë£Œ
  Future<void> endMorningCall() async {
    try {
      print('ğŸ›‘ ëª¨ë‹ì½œ ì¢…ë£Œ');
      
      // WebSocket ì—°ê²° ì¢…ë£Œ
      await _channel?.sink.close();
      _channel = null;
      _isConnected = false;
      
      // WebRTC ì •ë¦¬
      await _localStream?.dispose();
      await _remoteStream?.dispose();
      await _peerConnection?.close();
      
      _localStream = null;
      _remoteStream = null;
      _peerConnection = null;
      
      _isCallActive = false;
      _sessionId = null;
      
      onCallEnded?.call();
      
    } catch (e) {
      print('âŒ ëª¨ë‹ì½œ ì¢…ë£Œ ì˜¤ë¥˜: $e');
    }
  }

  /// í˜„ì¬ í†µí™” ìƒíƒœ
  bool get isCallActive => _isCallActive;
  
  /// í˜„ì¬ ì—°ê²° ìƒíƒœ
  bool get isConnected => _isConnected;
  
  /// ì„¸ì…˜ ID
  String? get sessionId => _sessionId;

  /// ì„œë¹„ìŠ¤ ì •ë¦¬
  void dispose() {
    endMorningCall();
  }
}
